# CorpInfoCrawler 

平台企业数据自动化采集脚本

##  项目简介

本项目是一个使用 Python 和 Selenium 实现的高度自动化数据采集脚本。其主要目标是自动登录特定的企业信息平台，智能识别并填写验证码，在登录后根据预设条件进行筛选和查询，并能循环进入每个企业条目的详情编辑页，抓取其中所有结构化的详细数据，最终将结果整合并导出为 Excel 文件。

该项目解决了手动登录、查询、复制、粘贴大量企业数据所带来的效率低下和重复性劳动问题，实现了端到端（从登录到数据导出）的自动化流程。

##  主要功能

- ** 自动登录**: 自动填充用户名、密码，并处理登录流程。
- ** 验证码识别**: 集成 `ddddocr` 库，对登录验证码进行本地OCR识别，成功率高。
- ** 智能导航**:
    - 自动在复杂的下拉框中选择指定的辖区。
    - 自动点击查询按钮以刷新列表。
    - 自动设置每页显示条数为100条，以获取最大数据量。
- ** 健壮的循环抓取**:
    - 能够循环遍历列表中的每一个项目。
    - 智能处理因页面跳转导致的“过时元素引用” (Stale Element Reference) 问题。
    - 为循环中的单个项目失败提供错误处理，保证主程序不会轻易中断。
- ** 详尽数据提取**:
    - 点击列表项的“编辑”按钮，进入动态加载的详情信息表单。
    - 逐一解析并提取表单内**所有**可见字段
    - 对缺失的字段进行容错处理。
- ** Excel 导出**: 使用 `pandas` 库将所有抓取到的数据整理成规范的表格，并导出一个名为 `scraped_data.xlsx` 的 Excel 文件。

##  技术栈

- **Python 3.11** (或其它稳定的 Python 3 版本)
- **Selenium**: 核心浏览器自动化框架。
- **ddddocr**: 用于本地、离线的验证码OCR识别。
- **Pandas**: 用于数据处理和导出到 Excel。
- **Openpyxl**: 作为 Pandas 的依赖库，用于支持 `.xlsx` 格式的读写。

##  环境准备

在运行此脚本之前，请确保您的环境中已安装以下软件：

1.  **Python**: 建议使用 3.11 或 3.12 等稳定版本。
2.  **Google Chrome**: 请确保您安装了最新版的谷歌浏览器。
3.  **ChromeDriver**: 下载与您的 Chrome 浏览器版本**完全对应**的 ChromeDriver。
    - **官方下载地址**: [Chrome for Testing 可用性资讯主页](https://googlechromelabs.github.io/chrome-for-testing/)
    - 下载后，请将 `chromedriver.exe` 文件放置在本项目根目录下，或将其所在目录添加到系统的**环境变量 (PATH)** 中。

##  安装与配置

1.  **克隆或下载项目**:
    ```bash
    git clone https://github.com/lushi78778/CorpInfoCrawler.git
    # 或者直接下载 .py 文件
    ```

2.  **安装依赖库**:
    ```bash
    pip install -r requirements.txt
    ```

3.  **配置脚本**:
    打开 `main.py` (或你的主脚本文件)，在文件顶部的配置区域修改以下变量：
    ```python
    # ==============================================================================
    # --- 1. 请在此处配置 ---
    YOUR_USERNAME = "你的用户名"  # 替换为你的真实用户名
    YOUR_PASSWORD = "你的密码"   # 替换为你的真实密码
    ITEMS_TO_SCRAPE = 50        # 修改为你希望爬取的资料数量
    OUTPUT_FILENAME = "scraped_data.xlsx" # 自定义输出的Excel文件名
    # ==============================================================================
    ```

##  如何运行

1.  确保所有配置已完成。
2.  打开命令行 (CMD 或 PowerShell)，进入项目所在的文件夹。
3.  执行以下命令：
    ```bash
    python main.py
    ```
4.  脚本将自动启动浏览器，并开始执行所有自动化流程。请耐心等待其完成。

##  输出结果

脚本成功执行完毕后，项目文件夹中会生成一个 Excel 文件 (默认为 `scraped_data.xlsx`)。文件中的每一行代表一个企业主体，每一列代表该主体的一个详细信息字段。

---

##  **免责声明** 

在使用本软件前，请您务必仔细阅读、充分理解并同意以下所有条款。一旦您开始运行、修改或分发本软件的任何部分，即表示您已接受并同意本声明的全部内容。

1.  **用途限制**: 本项目代码仅供个人学习、技术交流和自动化流程研究使用。严禁用于任何形式的商业用途或非法活动。使用者不得利用本软件从事任何违反其所在地区、目标网站所在地区、以及任何已知或未知宇宙中相关法律法规的行为。

2.  **合法性与合规性**: 使用者必须独立承担因使用本软件而产生的所有法律责任。这包括但不限于遵守目标网站的《服务条款》(ToS)、《隐私政策》、robots.txt 协议以及任何与数据隐私、版权和计算机滥用相关的法律。对于因绕过反爬虫措施、侵犯数据隐私或造成目标服务器不稳定而引发的任何法律纠纷、经济损失或“数字幽灵”的纠缠，本软件的作者概不负责。

3.  **风险自负**: 本软件是一个依赖于特定网站前端结构的自动化工具。目标网站的任何微小改版（包括但不限于HTML结构、CSS类名、JavaScript逻辑的变更）都可能导致本软件部分或全部功能失效。作者不提供任何形式的明示或暗示的保证，包括但不限于对软件的可用性、准确性、可靠性或持续维护的承诺。您需自行承担软件“一觉醒来就罢工”的风险。

4.  **数据准确性**: 本软件仅作为数据的“搬运工”，不对所抓取数据的准确性、完整性、时效性或合法性负责。所有数据的最终解释权归数据来源方所有。因使用本软件抓取的数据而做出的任何决策（无论多么英明或愚蠢），其后果均由使用者自行承担。

5.  **最终解释权**: 在任何已知物理定律所及的维度及时空连续体中，本声明的最终解释权归作者所有。作者有权在不事先通知的情况下，随时修改本声明。任何因使用本软件导致的直接、间接、偶然、特殊、惩罚性或后果性的损害（包括但不限于您的电脑变成“烤面包机”、您的猫学会了量子纠缠、或因IP被封锁而错过了重要的线上会议），无论作者是否被告知该等损害的可能性，作者均不承担任何赔偿责任，直到宇宙热寂。

**简而言之：您的电脑，您的网络，您的行为，您的责任。祝您使用愉快。**